{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM38L4RgWW5YV6/Hm9nsNSX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rohit-Madhesiya/GenAI_KrishNaik/blob/main/Text_Preprocessing_Stemming_Lemmatization_Stopwords.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming:** \\\n",
        "Stemming is the process of reducing a word to its word stem that addixes to suffixes and prefixes or to the roots of words known as ***lemma***. \\\n",
        "Stemming is important in natural language understanding (NLU) and natural language processing (NLP)."
      ],
      "metadata": {
        "id": "1QFoUcSdrt42"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "q0_Lotv-rmq_"
      },
      "outputs": [],
      "source": [
        "# Classification Problem\n",
        "# Comments of product is a positive review or negative review\n",
        "# Reviews can have words like --->{eating, eat, eaten}->eat(word stem),{going,gone,goes}->go(word stem)\n",
        "\n",
        "words=[\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalize\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PorterStemmer**"
      ],
      "metadata": {
        "id": "LpcjZLFKtNYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "8NGDzIRks4Kq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemming=PorterStemmer()"
      ],
      "metadata": {
        "id": "wb2IPqgltTnm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word+\"--->\"+stemming.stem(word))\n",
        "# Some words are not changed or changed badly-->major disadvantage of stemming"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOVF3xqhtW1e",
        "outputId": "bf69d475-dc1f-4a70-8f86-f3971f119192"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating--->eat\n",
            "eats--->eat\n",
            "eaten--->eaten\n",
            "writing--->write\n",
            "writes--->write\n",
            "programming--->program\n",
            "programs--->program\n",
            "history--->histori\n",
            "finally--->final\n",
            "finalize--->final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemming.stem('congratulations')\n",
        "# It is changing the meaning of the word-->Disadvantage ------>\n",
        "# These all will be fixed with the help of Lemmatization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "gdJNvxB1tdtV",
        "outputId": "b9d4520e-4277-411a-c62a-eed3a4aea516"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'congratul'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemming.stem('sitting')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "NJk05XlCt57Z",
        "outputId": "8b2cea74-3e1e-49e2-cc67-d37e05652227"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sit'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RegexpStemmer Class:** \\\n",
        "NLTK has RegexpStemmer class with the help of which we can easily implement Regular Expression Stemmer algorithms. \\\n",
        "It basically takes a single regular expression and removes any prefix or suffix that matches the expression."
      ],
      "metadata": {
        "id": "oAKo6fbFuTb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer"
      ],
      "metadata": {
        "id": "ogH0t-PDuDO4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stemmer=RegexpStemmer('ing$|s$|e$|able$', min=4) #whatever in the last of the word, just remove it\n",
        "# words having ing_/s_/e_/able_ remove these characters from the words"
      ],
      "metadata": {
        "id": "2RaQJjlguxuw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stemmer.stem('eating')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ZORNBZWeu0dj",
        "outputId": "87371413-0996-42f4-c984-fb59538e439b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stemmer.stem('ingeating') #here it will not work"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "iV9atBF1vHHs",
        "outputId": "1cfaac20-4cb8-495f-f3a9-007b567ca969"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ingeat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Snowball Stemmer:** \\\n"
      ],
      "metadata": {
        "id": "Ho7_DeZlvs-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer"
      ],
      "metadata": {
        "id": "AKwOdyBIviEW"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snowball_stemmer=SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "ubohJOEkv666"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word+\"--->\"+snowball_stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pg_FmqQav-iI",
        "outputId": "094ce8b1-bdee-4ff6-d728-ad73ed863a8a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating--->eat\n",
            "eats--->eat\n",
            "eaten--->eaten\n",
            "writing--->write\n",
            "writes--->write\n",
            "programming--->program\n",
            "programs--->program\n",
            "history--->histori\n",
            "finally--->final\n",
            "finalize--->final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemming.stem('fairly'),stemming.stem('sportingly')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUav5zalwOB-",
        "outputId": "673f75da-e3ed-4617-a26a-3fdc16b68384"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fairli', 'sportingli')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "snowball_stemmer.stem('fairly'),snowball_stemmer.stem('sportingly')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MH6bbI3jwW9p",
        "outputId": "6ed5cac5-5fe7-4304-9ca8-7ed773056cd6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fair', 'sport')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Snowball Stemmer performs better than Porter Stemmer but still not efficient\n",
        "snowball_stemmer.stem('goes')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "U5KkapOSwcjf",
        "outputId": "5f2fba79-4642-4b3e-b90f-f2c5010802f2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'goe'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Lemmatization:**"
      ],
      "metadata": {
        "id": "kz_wHmC4xFLd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Wordnet Lemmatizer:** \\\n",
        "Lemmatization technique is like stemming. The output we will get after lemmatization is called ***'lemma'***, which is a *'root word'* rather than root stem, the output of stemming. \\\n",
        "After lemmatization, we will be getting a valid word that means the same thing. \\\n",
        "\n",
        "NLTK provides WordNetLemmatizer class which is a thin wrapper around the wordnet corpus. This class uses morphy() function to the WordNet CorpusReader class to find a lemma.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Use Cases:** \\\n",
        "Q&A, Chatbots,  Text Summarization"
      ],
      "metadata": {
        "id": "lvWu_051zkOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3DPOyyJxIMO",
        "outputId": "7b9aba61-6834-46dc-b487-962e7a03660a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer=WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "VP_jaWYH1I_V"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "POS- Noun-n\n",
        "verb-v\n",
        "adjective-a\n",
        "adverb-r\n",
        "'''\n",
        "lemmatizer.lemmatize(\"going\",pos='v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "6UShU2l31J1C",
        "outputId": "289011ce-a5d0-4a7c-9f7b-b0ae93ecc233"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'go'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word+\"---->\"+lemmatizer.lemmatize(word,pos='n'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQzGhoJ51PgE",
        "outputId": "699028bc-0660-4a50-d67d-f4e8e198e74b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating---->eating\n",
            "eats---->eats\n",
            "eaten---->eaten\n",
            "writing---->writing\n",
            "writes---->writes\n",
            "programming---->programming\n",
            "programs---->program\n",
            "history---->history\n",
            "finally---->finally\n",
            "finalize---->finalize\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word+\"---->\"+lemmatizer.lemmatize(word,pos='v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7eluDSy10tX",
        "outputId": "5f8ad748-c76a-447d-9bd4-e8f2da9d28cb"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating---->eat\n",
            "eats---->eat\n",
            "eaten---->eat\n",
            "writing---->write\n",
            "writes---->write\n",
            "programming---->program\n",
            "programs---->program\n",
            "history---->history\n",
            "finally---->finally\n",
            "finalize---->finalize\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize('goes',pos='v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "8uXNE9gc143b",
        "outputId": "696888d0-c05f-4afe-b5b0-a3e44ab525cd"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'go'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize('fairly'), lemmatizer.lemmatize('sportingly')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVZuLY5sAERL",
        "outputId": "26003fec-601a-4c39-f514-da8ba7c30d31"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fairly', 'sportingly')"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Stopwords:**"
      ],
      "metadata": {
        "id": "YilSV2KTAn1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Speech of Dr. APJ Abdul Kalam\n",
        "paragraph=\"\"\"Greeting everyone. Today, I am here to deliver a speech on APJ Abdul Kalam.\n",
        "Dr APJ Abdul Kalam’s full name was Avul Pakir Zainuldeben Abdul Kalam,\n",
        "very few people know him by his full name as he was mostly addressed as ‘Missile Man of India’\n",
        "and ‘People’s President’. He was born into a very poor family in Rameswaram on October 15, 1931.\n",
        "Since childhood, he enjoyed flying, and was equally curious to know how birds fly in the air?\n",
        "He was very intelligent and enjoyed reading, but his family did not have sufficient income for his school fees,\n",
        "so to support his education, he would wake up early in the morning and ride a bicycle 3 kilometres from home to collect newspapers and sell them.\n",
        "He was admitted to St. Joseph's College, Tiruchirapalli,\n",
        "and later he went on to complete a degree in physics in 1954\n",
        "and then studied at the Madras Institute of Technology and graduated in aeronautical engineering in 1955.\n",
        "Since his childhood, Dr Abdul Alam wanted to be a pilot but couldn’t make his dream come true.\n",
        "He learned from his mistakes and accomplished numerous achievements in his life.\n",
        "After completing his degree, Abdul Kalam entered the Defense Department of India.\n",
        "He has been one of the key figures in building the nuclear capabilities of India.\n",
        "APJ Abdul Kalam was appointed to the Indian Ministry of Defense as a Technical Advisor in 1992, after which he served with DRDO and ISRO,\n",
        "the country's largest organization. Considered a national hero for successful nuclear tests in 1998,\n",
        "a second successful nuclear test was conducted in Pokhran the same year under his supervision,\n",
        "after which India was included in the list of nuclear-powered nations.\n",
        "Abdul Kalam has been active in all space programs and development programs in India as a scientist.\n",
        "For developing India's Agni missile, Kalam was called 'Missile Man.'Abdul Kalam made a special technological and scientific contribution,\n",
        "for which, along with Bharat Ratna, India's highest honour, he was awarded the Padma Bhushan, Padam Vibhushan, etc.\n",
        "He was also awarded an honorary doctorate by more than 30 universities in the world for the same.\n",
        "In 2002, he was elected President of India and was the country's first scientist and non-political president.\n",
        "He visited many countries during his tenure as President and\n",
        "led India's youth through his lectures and encouraged them to move forward.\n",
        "‘My vision for India’ was a Famous Speech of APJ Abdul Kalam delivered at IIT Hyderabad in 2011,\n",
        "and is to this day my favourite speech. His far-reaching thinking gave India's growth a fresh path and became the youth's inspiration.\n",
        "Dr Abdul Kalam died on July 27, 2015, from an apparent cardiac arrest while delivering a lecture at IIM Shillong at the age of 83.\n",
        " He spent his entire life in service and inspiration for the nation and the youth, and his death is also while addressing the youth.\n",
        " His death is a never-ending loss to the country.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Cm6S64pjAqjo"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "Y7gIawimDKAs"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXeL7fgeDN6r",
        "outputId": "0d8f4e05-2dca-4d65-d85d-00643a83afc4"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7hlhZ0JDS2_",
        "outputId": "4cbd8e94-afa9-44c2-f87d-261d4d150300"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('german')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_frut2WDbWm",
        "outputId": "1ab30797-2afa-4c2d-daa6-31028927f821"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aber',\n",
              " 'alle',\n",
              " 'allem',\n",
              " 'allen',\n",
              " 'aller',\n",
              " 'alles',\n",
              " 'als',\n",
              " 'also',\n",
              " 'am',\n",
              " 'an',\n",
              " 'ander',\n",
              " 'andere',\n",
              " 'anderem',\n",
              " 'anderen',\n",
              " 'anderer',\n",
              " 'anderes',\n",
              " 'anderm',\n",
              " 'andern',\n",
              " 'anderr',\n",
              " 'anders',\n",
              " 'auch',\n",
              " 'auf',\n",
              " 'aus',\n",
              " 'bei',\n",
              " 'bin',\n",
              " 'bis',\n",
              " 'bist',\n",
              " 'da',\n",
              " 'damit',\n",
              " 'dann',\n",
              " 'der',\n",
              " 'den',\n",
              " 'des',\n",
              " 'dem',\n",
              " 'die',\n",
              " 'das',\n",
              " 'dass',\n",
              " 'daß',\n",
              " 'derselbe',\n",
              " 'derselben',\n",
              " 'denselben',\n",
              " 'desselben',\n",
              " 'demselben',\n",
              " 'dieselbe',\n",
              " 'dieselben',\n",
              " 'dasselbe',\n",
              " 'dazu',\n",
              " 'dein',\n",
              " 'deine',\n",
              " 'deinem',\n",
              " 'deinen',\n",
              " 'deiner',\n",
              " 'deines',\n",
              " 'denn',\n",
              " 'derer',\n",
              " 'dessen',\n",
              " 'dich',\n",
              " 'dir',\n",
              " 'du',\n",
              " 'dies',\n",
              " 'diese',\n",
              " 'diesem',\n",
              " 'diesen',\n",
              " 'dieser',\n",
              " 'dieses',\n",
              " 'doch',\n",
              " 'dort',\n",
              " 'durch',\n",
              " 'ein',\n",
              " 'eine',\n",
              " 'einem',\n",
              " 'einen',\n",
              " 'einer',\n",
              " 'eines',\n",
              " 'einig',\n",
              " 'einige',\n",
              " 'einigem',\n",
              " 'einigen',\n",
              " 'einiger',\n",
              " 'einiges',\n",
              " 'einmal',\n",
              " 'er',\n",
              " 'ihn',\n",
              " 'ihm',\n",
              " 'es',\n",
              " 'etwas',\n",
              " 'euer',\n",
              " 'eure',\n",
              " 'eurem',\n",
              " 'euren',\n",
              " 'eurer',\n",
              " 'eures',\n",
              " 'für',\n",
              " 'gegen',\n",
              " 'gewesen',\n",
              " 'hab',\n",
              " 'habe',\n",
              " 'haben',\n",
              " 'hat',\n",
              " 'hatte',\n",
              " 'hatten',\n",
              " 'hier',\n",
              " 'hin',\n",
              " 'hinter',\n",
              " 'ich',\n",
              " 'mich',\n",
              " 'mir',\n",
              " 'ihr',\n",
              " 'ihre',\n",
              " 'ihrem',\n",
              " 'ihren',\n",
              " 'ihrer',\n",
              " 'ihres',\n",
              " 'euch',\n",
              " 'im',\n",
              " 'in',\n",
              " 'indem',\n",
              " 'ins',\n",
              " 'ist',\n",
              " 'jede',\n",
              " 'jedem',\n",
              " 'jeden',\n",
              " 'jeder',\n",
              " 'jedes',\n",
              " 'jene',\n",
              " 'jenem',\n",
              " 'jenen',\n",
              " 'jener',\n",
              " 'jenes',\n",
              " 'jetzt',\n",
              " 'kann',\n",
              " 'kein',\n",
              " 'keine',\n",
              " 'keinem',\n",
              " 'keinen',\n",
              " 'keiner',\n",
              " 'keines',\n",
              " 'können',\n",
              " 'könnte',\n",
              " 'machen',\n",
              " 'man',\n",
              " 'manche',\n",
              " 'manchem',\n",
              " 'manchen',\n",
              " 'mancher',\n",
              " 'manches',\n",
              " 'mein',\n",
              " 'meine',\n",
              " 'meinem',\n",
              " 'meinen',\n",
              " 'meiner',\n",
              " 'meines',\n",
              " 'mit',\n",
              " 'muss',\n",
              " 'musste',\n",
              " 'nach',\n",
              " 'nicht',\n",
              " 'nichts',\n",
              " 'noch',\n",
              " 'nun',\n",
              " 'nur',\n",
              " 'ob',\n",
              " 'oder',\n",
              " 'ohne',\n",
              " 'sehr',\n",
              " 'sein',\n",
              " 'seine',\n",
              " 'seinem',\n",
              " 'seinen',\n",
              " 'seiner',\n",
              " 'seines',\n",
              " 'selbst',\n",
              " 'sich',\n",
              " 'sie',\n",
              " 'ihnen',\n",
              " 'sind',\n",
              " 'so',\n",
              " 'solche',\n",
              " 'solchem',\n",
              " 'solchen',\n",
              " 'solcher',\n",
              " 'solches',\n",
              " 'soll',\n",
              " 'sollte',\n",
              " 'sondern',\n",
              " 'sonst',\n",
              " 'über',\n",
              " 'um',\n",
              " 'und',\n",
              " 'uns',\n",
              " 'unsere',\n",
              " 'unserem',\n",
              " 'unseren',\n",
              " 'unser',\n",
              " 'unseres',\n",
              " 'unter',\n",
              " 'viel',\n",
              " 'vom',\n",
              " 'von',\n",
              " 'vor',\n",
              " 'während',\n",
              " 'war',\n",
              " 'waren',\n",
              " 'warst',\n",
              " 'was',\n",
              " 'weg',\n",
              " 'weil',\n",
              " 'weiter',\n",
              " 'welche',\n",
              " 'welchem',\n",
              " 'welchen',\n",
              " 'welcher',\n",
              " 'welches',\n",
              " 'wenn',\n",
              " 'werde',\n",
              " 'werden',\n",
              " 'wie',\n",
              " 'wieder',\n",
              " 'will',\n",
              " 'wir',\n",
              " 'wird',\n",
              " 'wirst',\n",
              " 'wo',\n",
              " 'wollen',\n",
              " 'wollte',\n",
              " 'würde',\n",
              " 'würden',\n",
              " 'zu',\n",
              " 'zum',\n",
              " 'zur',\n",
              " 'zwar',\n",
              " 'zwischen']"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Stopwords and Filter using PorterStemmer**"
      ],
      "metadata": {
        "id": "oKAmki_hGQPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "c7y6CqZ2Dm3z"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer=PorterStemmer()"
      ],
      "metadata": {
        "id": "WKkL8b4PDrC-"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKc4zUm_EHOg",
        "outputId": "dc3a4ae5-317c-4de7-9942-f5f39ff521bd"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents=nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "5nUrbBRED4dD"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QxKd8OvD9y5",
        "outputId": "66208034-2706-44c0-ee17-d0f0f20704ba"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Stopwords and Filter and then Applying Stemming\n",
        "for i in range(len(documents)):\n",
        "  words=nltk.word_tokenize(documents[i])\n",
        "  words=[stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "  documents[i]=' '.join(words) #converting all the list of words into sentences using join"
      ],
      "metadata": {
        "id": "z0bd2o1GEWaQ"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihg4dIl4FGpS",
        "outputId": "1d453d58-9343-4dd6-fa3f-fada6d808cb7"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['greet everyon .',\n",
              " 'today , deliv speech apj abdul kalam .',\n",
              " 'dr apj abdul kalam ’ full name avul pakir zainuldeben abdul kalam , peopl know full name mostli address ‘ missil man india ’ ‘ peopl ’ presid ’ .',\n",
              " 'born poor famili rameswaram octob 15 , 1931 .',\n",
              " 'sinc childhood , enjoy fli , equal curiou know bird fli air ?',\n",
              " 'intellig enjoy read , famili suffici incom school fee , support educ , would wake earli morn ride bicycl 3 kilometr home collect newspap sell .',\n",
              " \"admit st. joseph 's colleg , tiruchirap , later went complet degr physic 1954 studi madra institut technolog graduat aeronaut engin 1955 .\",\n",
              " 'sinc childhood , dr abdul alam want pilot ’ make dream come true .',\n",
              " 'learn mistak accomplish numer achiev life .',\n",
              " 'complet degr , abdul kalam enter defen depart india .',\n",
              " 'one key figur build nuclear capabl india .',\n",
              " \"apj abdul kalam appoint indian ministri defen technic advisor 1992 , serv drdo isro , countri 's largest organ .\",\n",
              " 'consid nation hero success nuclear test 1998 , second success nuclear test conduct pokhran year supervi , india includ list nuclear-pow nation .',\n",
              " 'abdul kalam activ space program develop program india scientist .',\n",
              " \"develop india 's agni missil , kalam call 'missil man .\",\n",
              " \"'abdul kalam made special technolog scientif contribut , , along bharat ratna , india 's highest honour , award padma bhushan , padam vibhushan , etc .\",\n",
              " 'also award honorari doctor 30 univ world .',\n",
              " \"2002 , elect presid india countri 's first scientist non-polit presid .\",\n",
              " \"visit mani countri tenur presid led india 's youth lectur encourag move forward .\",\n",
              " '‘ vision india ’ famou speech apj abdul kalam deliv iit hyderabad 2011 , day favourit speech .',\n",
              " \"hi far-reach think gave india 's growth fresh path becam youth 's inspir .\",\n",
              " 'dr abdul kalam die juli 27 , 2015 , appar cardiac arrest deliv lectur iim shillong age 83 .',\n",
              " 'spent entir life servic inspir nation youth , death also address youth .',\n",
              " 'hi death never-end loss countri .']"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Stopwords and Filter using SnowballStemmer**"
      ],
      "metadata": {
        "id": "9WFasXDnGYBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snowball_stemmer=SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "O8zzwqrZFJ9F"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Stopwords and Filter using Snowball Stemmer\n",
        "for i in range(len(documents)):\n",
        "  words=nltk.word_tokenize(documents[i])\n",
        "  words=[snowball_stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "  documents[i]=' '.join(words) #converting the list of words into sentences"
      ],
      "metadata": {
        "id": "zvcZd8AtFUxq"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKXhIoAvFxHZ",
        "outputId": "6bf1c081-948b-4c8d-f710-e8e778b56fba"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['greet everyon .',\n",
              " 'today , deliv speech apj abdul kalam .',\n",
              " 'dr apj abdul kalam ’ full name avul pakir zainuldeben abdul kalam , peopl know full name most address ‘ missil man india ’ ‘ peopl ’ presid ’ .',\n",
              " 'born poor famili rameswaram octob 15 , 1931 .',\n",
              " 'sinc childhood , enjoy fli , equal curiou know bird fli air ?',\n",
              " 'intellig enjoy read , famili suffici incom school fee , support educ , would wake ear morn ride bicycl 3 kilometr home collect newspap sell .',\n",
              " \"admit st. joseph 's colleg , tiruchirap , later went complet degr physic 1954 studi madra institut technolog graduat aeronaut engin 1955 .\",\n",
              " 'sinc childhood , dr abdul alam want pilot ’ make dream come true .',\n",
              " 'learn mistak accomplish numer achiev life .',\n",
              " 'complet degr , abdul kalam enter defen depart india .',\n",
              " 'one key figur build nuclear capabl india .',\n",
              " \"apj abdul kalam appoint indian ministri defen technic advisor 1992 , serv drdo isro , countri 's largest organ .\",\n",
              " 'consid nation hero success nuclear test 1998 , second success nuclear test conduct pokhran year supervi , india includ list nuclear-pow nation .',\n",
              " 'abdul kalam activ space program develop program india scientist .',\n",
              " \"develop india 's agni missil , kalam call missil man .\",\n",
              " \"abdul kalam made special technolog scientif contribut , , along bharat ratna , india 's highest honour , award padma bhushan , padam vibhushan , etc .\",\n",
              " 'also award honorari doctor 30 univ world .',\n",
              " \"2002 , elect presid india countri 's first scientist non-polit presid .\",\n",
              " \"visit mani countri tenur presid led india 's youth lectur encourag move forward .\",\n",
              " '‘ vision india ’ famou speech apj abdul kalam deliv iit hyderabad 2011 , day favourit speech .',\n",
              " \"hi far-reach think gave india 's growth fresh path becam youth 's inspir .\",\n",
              " 'dr abdul kalam die juli 27 , 2015 , appar cardiac arrest deliv lectur iim shillong age 83 .',\n",
              " 'spent entir life servic inspir nation youth , death also address youth .',\n",
              " 'hi death never-end loss countri .']"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Stopwords and Filter using WordNetLemmatizer**"
      ],
      "metadata": {
        "id": "PCilgNnmGjid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "z_S3hxp4F41P"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer=WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "LZohI7RUGtpZ"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Stopwords and Filter using Lemmatizer\n",
        "for i in range(len(documents)):\n",
        "  documents[i]=documents[i].lower()\n",
        "  words=nltk.word_tokenize(documents[i])\n",
        "  words=[lemmatizer.lemmatize(word,pos='v') for word in words if word not in set(stopwords.words('english'))]\n",
        "  documents[i]=' '.join(words)"
      ],
      "metadata": {
        "id": "0osfOdLEGvoc"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvPGShk6HVGu",
        "outputId": "3dcb1511-c47f-4073-c23e-fa382144fefa"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['greet everyone .',\n",
              " 'today , deliver speech apj abdul kalam .',\n",
              " 'dr apj abdul kalam ’ full name avul pakir zainuldeben abdul kalam , people know full name mostly address ‘ missile man india ’ ‘ people ’ president ’ .',\n",
              " 'bear poor family rameswaram october 15 , 1931 .',\n",
              " 'since childhood , enjoy fly , equally curious know bird fly air ?',\n",
              " 'intelligent enjoy read , family sufficient income school fee , support education , would wake early morning ride bicycle 3 kilometre home collect newspaper sell .',\n",
              " \"admit st. joseph 's college , tiruchirapalli , later go complete degree physic 1954 study madras institute technology graduate aeronautical engineer 1955 .\",\n",
              " 'since childhood , dr abdul alam want pilot ’ make dream come true .',\n",
              " 'learn mistake accomplish numerous achievement life .',\n",
              " 'complete degree , abdul kalam enter defense department india .',\n",
              " 'one key figure build nuclear capability india .',\n",
              " \"apj abdul kalam appoint indian ministry defense technical advisor 1992 , serve drdo isro , country 's largest organization .\",\n",
              " 'consider national hero successful nuclear test 1998 , second successful nuclear test conduct pokhran year supervision , india include list nuclear-powered nation .',\n",
              " 'abdul kalam active space program development program india scientist .',\n",
              " \"develop india 's agni missile , kalam call 'missile man .\",\n",
              " \"'abdul kalam make special technological scientific contribution , , along bharat ratna , india 's highest honour , award padma bhushan , padam vibhushan , etc .\",\n",
              " 'also award honorary doctorate 30 university world .',\n",
              " \"2002 , elect president india country 's first scientist non-political president .\",\n",
              " \"visit many country tenure president lead india 's youth lecture encourage move forward .\",\n",
              " '‘ vision india ’ famous speech apj abdul kalam deliver iit hyderabad 2011 , day favourite speech .',\n",
              " \"far-reaching think give india 's growth fresh path become youth 's inspiration .\",\n",
              " 'dr abdul kalam die july 27 , 2015 , apparent cardiac arrest deliver lecture iim shillong age 83 .',\n",
              " 'spend entire life service inspiration nation youth , death also address youth .',\n",
              " 'death never-ending loss country .']"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9_18oByZHeSy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}