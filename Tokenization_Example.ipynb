{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5LypFG8TSnHr8l8RXZzni",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rohit-Madhesiya/GenAI_KrishNaik/blob/main/Tokenization_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8hNs6AelSxM",
        "outputId": "c0c76fde-f741-4712-9df2-dff3e65dcfde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus=\"\"\"Hello Welcome to the world of NLP's.\n",
        "Please do watch the entire course! to become expert in Natural Language Processing.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "GZPYcEcclYlc"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JH6DqKCrl4LP",
        "outputId": "3229eb15-178e-4ddc-f661-aaf073ad4f28"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Welcome to the world of NLP's.\n",
            "Please do watch the entire course! to become expert in Natural Language Processing.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization:** \\\n",
        "Step-1: Sentence-->Paragraphs"
      ],
      "metadata": {
        "id": "imafI1pZl6m6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sent_tokenize is used for tokenizing the sentence\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6xUwgptl5Z8",
        "outputId": "3f7eaa20-b768-4d4e-9744-8241e30291c6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents=sent_tokenize(corpus)"
      ],
      "metadata": {
        "id": "NFJv98cCmGMl"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(documents))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bb4hD_lmISx",
        "outputId": "050d5dc7-7a34-4cf8-96ff-2c7ceb623587"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in documents:\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZFpaYD6m5UL",
        "outputId": "8fc8ce2c-c339-4442-cd71-a1b1167c6f38"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Welcome to the world of NLP's.\n",
            "Please do watch the entire course!\n",
            "to become expert in Natural Language Processing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization:** \\\n",
        "Paragraph-->Words \\\n",
        "Sentence-->Words"
      ],
      "metadata": {
        "id": "23NwBOaNm-WZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# word_tokenize is used for tokenizing the word as token\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "ghZ9fcNom84C"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PKizqBwnJOU",
        "outputId": "4b518717-693a-46bf-ab3f-674e826f351a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'Welcome',\n",
              " 'to',\n",
              " 'the',\n",
              " 'world',\n",
              " 'of',\n",
              " 'NLP',\n",
              " \"'s\",\n",
              " '.',\n",
              " 'Please',\n",
              " 'do',\n",
              " 'watch',\n",
              " 'the',\n",
              " 'entire',\n",
              " 'course',\n",
              " '!',\n",
              " 'to',\n",
              " 'become',\n",
              " 'expert',\n",
              " 'in',\n",
              " 'Natural',\n",
              " 'Language',\n",
              " 'Processing',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in documents:\n",
        "  print(word_tokenize(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQoCdXX5nLi4",
        "outputId": "a5273e9c-080d-46c7-a2c0-a55d28dcae43"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'Welcome', 'to', 'the', 'world', 'of', 'NLP', \"'s\", '.']\n",
            "['Please', 'do', 'watch', 'the', 'entire', 'course', '!']\n",
            "['to', 'become', 'expert', 'in', 'Natural', 'Language', 'Processing', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Punctuation like 's will also be tokenized with this library\n",
        "from nltk.tokenize import wordpunct_tokenize"
      ],
      "metadata": {
        "id": "9pUFjFG5niDy"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordpunct_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUCWWmdonoh4",
        "outputId": "5b572641-1940-4f12-ab4a-70e4bbe0770d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'Welcome',\n",
              " 'to',\n",
              " 'the',\n",
              " 'world',\n",
              " 'of',\n",
              " 'NLP',\n",
              " \"'\",\n",
              " 's',\n",
              " '.',\n",
              " 'Please',\n",
              " 'do',\n",
              " 'watch',\n",
              " 'the',\n",
              " 'entire',\n",
              " 'course',\n",
              " '!',\n",
              " 'to',\n",
              " 'become',\n",
              " 'expert',\n",
              " 'in',\n",
              " 'Natural',\n",
              " 'Language',\n",
              " 'Processing',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# With this library fullstop(.) btwn the words will not be treated as separate word\n",
        "from nltk.tokenize import TreebankWordTokenizer"
      ],
      "metadata": {
        "id": "vonB37jMntdS"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=TreebankWordTokenizer()"
      ],
      "metadata": {
        "id": "WQ7fVjO_oINb"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N22tto9soKH8",
        "outputId": "cc45ee82-b9a1-45c6-e89b-a71c48013859"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'Welcome',\n",
              " 'to',\n",
              " 'the',\n",
              " 'world',\n",
              " 'of',\n",
              " \"NLP's.\",\n",
              " 'Please',\n",
              " 'do',\n",
              " 'watch',\n",
              " 'the',\n",
              " 'entire',\n",
              " 'course',\n",
              " '!',\n",
              " 'to',\n",
              " 'become',\n",
              " 'expert',\n",
              " 'in',\n",
              " 'Natural',\n",
              " 'Language',\n",
              " 'Processing',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qkzy4rDioP7g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}